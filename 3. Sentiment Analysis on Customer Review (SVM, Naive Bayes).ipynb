{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 情感分析项目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "本项目的目标是基于用户提供的评论，通过算法自动去判断其评论是正面的还是负面的情感。比如给定一个用户的评论：\n",
    "- 评论1： “我特别喜欢这个电器，我已经用了3个月，一点问题都没有！”\n",
    "- 评论2： “我从这家淘宝店卖的东西不到一周就开始坏掉了，强烈建议不要买，真实浪费钱”\n",
    "\n",
    "对于这两个评论，第一个明显是正面的，第二个是负面的。 我们希望搭建一个AI算法能够自动帮我们识别出评论是正面还是负面。\n",
    "\n",
    "情感分析的应用场景非常丰富，也是NLP技术在不同场景中落地的典范。比如对于一个证券领域，作为股民，其实比较关注舆论的变化，这个时候如果能有一个AI算法自动给网络上的舆论做正负面判断，然后把所有相关的结论再整合，这样我们可以根据这些大众的舆论，辅助做买卖的决策。 另外，在电商领域评论无处不在，而且评论已经成为影响用户购买决策的非常重要的因素，所以如果AI系统能够自动分析其情感，则后续可以做很多有意思的应用。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "情感分析是文本处理领域经典的问题。整个系统一般会包括几个模块：\n",
    "- 数据的抓取： 通过爬虫的技术去网络抓取相关文本数据\n",
    "- 数据的清洗/预处理：在本文中一般需要去掉无用的信息，比如各种标签（HTML标签），标点符号，停用词等等\n",
    "- 把文本信息转换成向量： 这也成为特征工程，文本本身是不能作为模型的输入，只有数字（比如向量）才能成为模型的输入。所以进入模型之前，任何的信号都需要转换成模型可识别的数字信号（数字，向量，矩阵，张量...)\n",
    "- 选择合适的模型以及合适的评估方法。 对于情感分析来说，这是二分类问题（或者三分类：正面，负面，中性），所以需要采用分类算法比如逻辑回归，朴素贝叶斯，神经网络，SVM等等。另外，我们需要选择合适的评估方法，比如对于一个应用，我们是关注准确率呢，还是关注召回率呢？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "在本次项目中，我们已经给定了训练数据和测试数据，它们分别是 ``train.positive.txt``, ``train.negative.txt``， ``test_combined.txt``. 请注意训练数据和测试数据的格式不一样，详情请见文件内容。 整个项目你需要完成以下步骤：\n",
    "\n",
    "数据的读取以及清洗： 从给定的.txt中读取内容，并做一些数据清洗，这里需要做几个工作： \n",
    "- （1） 文本的读取，需要把字符串内容读进来。 \n",
    "- （2）去掉无用的字符比如标点符号，多余的空格，换行符等 \n",
    "- （3） 把文本转换成``TF-IDF``向量： 这部分直接可以利用sklearn提供的``TfidfVectorizer``类来做。\n",
    "- （4） 利用逻辑回归等模型来做分类，并通过交叉验证选择最合适的超参数\n",
    "\n",
    "项目中需要用到的数据：\n",
    "- ``train.positive.txt``, ``train.negative.txt``， ``test_combined.txt``： 训练和测试数据\n",
    "- ``stopwords.txt``： 停用词库\n",
    "\n",
    "\n",
    "你需要完成的部分为标记为`TODO`的部分。 \n",
    "\n",
    "另外，提交作业时候的注意点：\n",
    "> 1. 不要试图去创建另外一个.ipynb文件，所有的程序需要在`starter_code.ipynb`里面实现。很多的模块已经帮你写好，不要试图去修改已经定义好的函数以及名字。 当然，自己可以按需求来创建新的函数。但一定要按照给定的框架来写程序，不然判作业的时候会出现很多问题。 \n",
    "> 2. 上传作业的时候把整个文件解压成.zip文件（不要.rar格式），请不要上传图片文件，其他的都需要上传包括`README.md`。\n",
    "> 3. 确保程序能够正常运行，我们支持的环境是`Python 3`,  千万不要使用`Python 2`\n",
    "> 4. 上传前一定要确保完整性，批改过一次的作业我们不会再重新批改，会作为最终的分数来对待。 \n",
    "> 5. 作业可以讨论，但请自己完成。让我们一起遵守贪心学院的`honor code`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. File Reading: 文本读取 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_file():\n",
    "    \"\"\"\n",
    "    读取训练数据和测试数据，并对它们做一些预处理\n",
    "    \"\"\"    \n",
    "    train_pos_file = \"./data/train.positive.txt\"\n",
    "    train_neg_file = \"./data/train.negative.txt\"\n",
    "    test_comb_file = \"./data/test.combined.txt\"\n",
    "    \n",
    "    # TODO: 读取文件部分，把具体的内容写入到变量里面\n",
    "    train_comments = []\n",
    "    train_labels = []\n",
    "    test_comments = []\n",
    "    test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "!head 3 './data/train.positive.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8065 8065\n",
      "['璇烽棶杩欐満涓嶆槸鏈変釜閬ユ帶鍣ㄧ殑鍚楋紵', '鍙戠煭淇＄壒鍒\\ue0a1笉鏂逛究锛佽儗鍚庣殑灞忓箷寰堝ぇ鐢ㄨ捣鏉ヤ笉鑸掓湇锛屾槸鎵嬭Е灞忕殑锛佸垏鎹㈠睆骞曞緢楹荤儲锛'] [1, 1]\n"
     ]
    }
   ],
   "source": [
    "train_pos_file = \"./data/train.positive.txt\"\n",
    "train_neg_file = \"./data/train.negative.txt\"\n",
    "import re\n",
    "def process_train(file_path):\n",
    "    file=open(file_path, 'r',encoding='gb18030', errors='ignore')\n",
    "    f1 = file.readlines()\n",
    "    text1=[]\n",
    "    for i in f1:\n",
    "        if (i!='</review>\\n' and i!='\\n'):\n",
    "            text1.append(i.replace('\\n',''))\n",
    "    text2=''.join(text1).split('<review id=\"')\n",
    "    #text2=''.join(text1).split('\">')\n",
    "    #print (text2)\n",
    "    #text3=text2.split('<review id=\"\"')\n",
    "    len(text2[1:])\n",
    "    train=[]\n",
    "    for i in text2[1:]:\n",
    "        #train.append(''.join(re.findall('\\w',i.replace('!','惊叹').replace('！','惊叹').replace('？',\"疑问\"))))\n",
    "        train.append(''.join(re.findall('\\D',i.replace(\">\",\"\").replace('\"','').replace('!','惊叹').replace('！','惊叹').replace('？',\"疑问\"))))\n",
    "    return train\n",
    "\n",
    "train_pos=process_train(train_pos_file)\n",
    "train_neg=process_train(train_neg_file)\n",
    "#print (train_pos[:2])\n",
    "#print (len(train_pos))\n",
    "#print (train_neg[:2])\n",
    "#print (len(train_neg))\n",
    "train_comments = train_pos+train_neg\n",
    "train_labels = [1] * len(train_pos) +[0]*len(train_neg)\n",
    "print (len(train_comments), len(train_labels))\n",
    "print (train_comments[:2], train_labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 0 ['  鎴戠粓浜庢壘鍒板悓閬撲腑浜哄暒锝烇綖锝烇綖浠庡垵涓\\ue15e紑濮嬶紝鎴戝氨宸茬粡鍠滄\\ue0bd涓婁簡michaeljackson.浣嗗悓瀛︿滑閮界敤閯欏し鐨勭溂鍏夌湅鎴戯紝浠栦滑浜轰负jackson鐨勬牱瀛愬彜鎬\\ue046敋鑷宠\\ue1e9锛備笐锛傦紟鎴戝綋鍦烘皵鏅曪紟浣嗙幇鍦ㄦ湁鍚岄亾涓\\ue15d汉浜嗭紝鎴戝ソ寮蹇冿紒锛侊紒michaeljacksonisthemostsuccessfulsingerintheworld惊叹惊叹~~~', '  鐪嬪畬宸叉槸娣卞\\ue641涓ょ偣锛屾垜鍗村潗鍦ㄧ數鑴戝墠鎯呴毦鑷\\ue046\\ue6e6锛岃繖鏄\\ue21b渶濂界殑缁撳眬銆傛儫鏈夊\\ue6e7姝わ紝灏辫\\ue180閭ｅ墠涓栦粖鐢熺殑绾犵粨灏卞仠鐣欏湪姝ゅ埢銆傚啀鐩搁㈡椂锛屾効浠栫殑浜虹敓涓嶅啀璁╀汉鍞忓槝锛屼粬浠\\ue102殑韬\\ue0a2績涔熶細鍙\\ue044眳涓澶勩傚彲鏄\\ue21d繕鏄\\ue21c棝蹇冧负杩欐牱鐨勪汉锛岃繖鏍风殑鐖扁︹'] [0, 1]\n"
     ]
    }
   ],
   "source": [
    "def process_test(file_path):\n",
    "    file=open(file_path, 'r',encoding='gb18030', errors='ignore')\n",
    "    f1 = file.readlines()\n",
    "    text1=[]\n",
    "    for i in f1:\n",
    "        if (i!='</review>\\n' and i!='\\n'):\n",
    "            text1.append(i.replace('\\n',''))\n",
    "    text2=''.join(text1).split('<review id=\"')\n",
    "    text3=''.join(text1).split('label=\"')\n",
    "    #print (text3[1:])\n",
    "    len(text2[1:])\n",
    "    comment=[]\n",
    "    label=[]\n",
    "    for i in text2[1:]:\n",
    "        #print (i)\n",
    "        comment.append(''.join(re.findall('\\D',i.replace(\">\",\"\").replace('\"','').replace(\"label=\",'').replace('!','惊叹').replace('！','惊叹').replace('？',\"疑问\"))))\n",
    "    for i in text3[1:]:\n",
    "        #print (i[0])\n",
    "        label.append(int(i[0]))\n",
    "    return comment, label\n",
    "test_comb_file = \"./data/test.combined.txt\"\n",
    "test_labels=[]\n",
    "test_label =[]\n",
    "test_comments=process_test(test_comb_file)[0]\n",
    "test_labels=process_test(test_comb_file)[1]\n",
    "print (len(test_comments), len(test_label), test_comments[:2], test_labels[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. Explorary Analysis: 做一些简单的可视化分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8065 2500\n"
     ]
    }
   ],
   "source": [
    "# 训练数据和测试数据大小\n",
    "print (len(train_comments), len(test_comments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> 这里有一个假设想验证。我觉得，如果一个评论是负面的，则用户留言时可能会长一些，因为对于负面的评论，用户很可能会把一些细节写得很清楚。但对于正面的评论，用户可能就只写“非常好”，这样的短句。我们想验证这个假设。 为了验证这个假设，打算画两个直方图，分别对正面的评论和负面的评论。 具体的做法是：1. 把正面和负面评论分别收集，之后分别对正面和负面评论画一个直方图。 2.  直方图的X轴是评论的长度，所以从是小到大的顺序。然后Y轴是对于每一个长度，出现了多少个正面或者负面的评论。 通过两个直方图的比较，即可以看出``评论``是否是一个靠谱的特征。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: 对于训练数据中的正负样本，分别画出一个histogram， histogram的x抽是每一个样本中字符串的长度，y轴是拥有这个长度的样本的百分比。\n",
    "#       并说出样本长度是否对情感有相关性 (需要先用到结巴分词)\n",
    "#       参考：https://baike.baidu.com/item/%E7%9B%B4%E6%96%B9%E5%9B%BE/1103834?fr=aladdin\n",
    "#       画饼状图参考： https://pythonspot.com/matplotlib-histogram/   \n",
    "#                   https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.hist.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "// TODO: 情感跟评论长度是否有相关性？\n",
    "\n",
    "// 你的答案.....\n",
    "\n",
    "<font color=red>这个可视化分析 我准备等文本预处理完了再做，把标点符号，特殊文字去掉之后算出来的句子含有的词的数量可能更加准确，图再第三题下面\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. 文本预处理\n",
    "> 在此部分需要做文本预处理方面的工作。 分为几大块：\n",
    "- ``去掉特殊符号``  比如#$.... 这部分的代码已经给出，不需要自己写\n",
    "- ``把数字转换成特殊单词`` 把数字转换成 \" NUM \"， 这部分需要写。 注意：NUM前面和后面加一个空格，这样可以保证之后分词时被分掉。\n",
    "- ``分词并过滤掉停用词`` 停用词库已经提供，需要读取停用词库，并按照此停用词库做过滤。 停用词库使用给定的文件：``stopwords.txt`` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 问题 怎么把数字换成‘NUM’？ 我在上面读入原始数据的时候因为要把 review id=1， 2， 3 中的数字去掉用了 正则表达式 中的 re.findall('\\D'）， 所以评论里的其他数字已经都被去掉了，怎么保留评论里的数字，又去掉id里面的数字呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_symbols(text):\n",
    "    \"\"\"\n",
    "    对特殊符号做一些处理，此部分已写好。如果不满意也可以自行改写，不记录分数。\n",
    "    \"\"\"\n",
    "    text = re.sub('[!！]+', \"!\", text)\n",
    "    text = re.sub('[?？]+', \"?\", text)\n",
    "    text = re.sub(\"[a-zA-Z#$%&\\'()*+,-./:;：<=>@，。★、…【】～《》>＂“”‘’[\\\\]^_`{|}~]+\", \"\", text)\n",
    "    #text = re.sub(\"[a-zA-Z#$%&\\'()*+,-./:;：<=>@，。★、…【】～《》>＂“”‘’[\\\\]^_`{|}~]+\", \" OOV \", text)\n",
    "    return re.sub(\"\\s+\", \" \", text)  \n",
    "\n",
    "\n",
    "# TODO：对于train_comments, test_comments进行字符串的处理，几个考虑的点：\n",
    "#   1. 去掉特殊符号\n",
    "#   2. 把数字转换成特殊字符或者单词\n",
    "#   3. 分词并做停用词过滤\n",
    "#   4. ... （或者其他）\n",
    "#\n",
    "#   需要注意的点是，由于评论数据本身很短，如果去掉的太多，很可能字符串长度变成0\n",
    "#   预处理部分，可以自行选择合适的方.\n",
    "train_comments_cleaned = [] \n",
    "for i in train_comments:\n",
    "    train_comments_cleaned.append(clean_symbols(i))\n",
    "    #a=[word for word in word_list if word not in stop_words]\n",
    "test_comments_cleaned = []\n",
    "for i in test_comments:\n",
    "    test_comments_cleaned.append(clean_symbols(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\jiabi\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.463 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "import jieba\n",
    "def cut_word(input_file):\n",
    "    # 基于jieba的分词  参考： https://github.com/fxsjy/jieba\n",
    "    cutted= []\n",
    "    for x in input_file:\n",
    "        cutted.append(' '.join(jieba.cut(x, cut_all=False)))\n",
    "    return cutted\n",
    "cutted_train_cleaned=cut_word(train_comments_cleaned)\n",
    "cutted_test_cleaned=cut_word(test_comments_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stopwords: 767\n",
      "number of stopwords: 768\n"
     ]
    }
   ],
   "source": [
    "stopwords_file = \"./data/stopwords.txt\"\n",
    "f=open(stopwords_file, 'r',encoding='gb18030', errors='ignore')\n",
    "text1= f.readlines()\n",
    "stop_words=[]\n",
    "for i in text1:\n",
    "    if (i!='\\n'):\n",
    "        stop_words.append(i.replace('\\n',''))\n",
    "print ('number of stopwords:' , len(stop_words))\n",
    "stop_words.append('的')\n",
    "print ('number of stopwords:' , len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['璇 烽 棶 杩 欐 満 涓 嶆 槸 鏈 変 釜 閬 ユ 帶 鍣 ㄧ 殑 鍚 楋 紵', '鍙 戠 煭 淇 ＄ 壒 鍒 \\ue0a1 笉 鏂 逛究 锛 佽 儗 鍚 庣 殑 灞 忓 箷 寰 堝 ぇ 鐢 ㄨ 捣 鏉 ヤ 笉 鑸 掓 湇 锛 屾 槸 鎵 嬭 Е 灞 忕 殑 锛 佸 垏 鎹 ㈠ 睆 骞 曞 緢 楹 荤 儲 锛']\n",
      "['  鎴 戠 粓 浜 庢 壘 鍒 板 悓 閬 撲 腑 浜 哄 暒 锝 烇 綖 锝 烇 綖 浠 庡 垵 涓 \\ue15e 紑 濮 嬶 紝 鎴 戝 氨宸 茬 粡 鍠 滄 \\ue0bd 涓 婁簡 浣 嗗 悓 瀛 ︿ 滑 閮 界 敤 閯 欏 し 鐨 勭 溂 鍏 夌 湅 鎴 戯 紝 浠 栦 滑 浜 轰负 鐨 勬 牱 瀛 愬 彜 鎬 \\ue046 敋 鑷宠 \\ue1e9 锛 備 笐 锛 傦 紟 鎴 戝 綋 鍦 烘 皵 鏅 曪 紟 浣 嗙 幇 鍦 ㄦ 湁 鍚 岄 亾 涓 \\ue15d 汉 浜 嗭 紝 鎴 戝 ソ 寮 蹇 冿 紒 锛 侊 紒 惊叹 惊叹', '  鐪 嬪 畬宸 叉 槸 娣 卞 \\ue641 涓 ょ 偣 锛 屾 垜 鍗 村 潗 鍦 ㄧ 數 鑴 戝 墠 鎯 呴 毦 鑷 \\ue046 \\ue6e6 锛 岃 繖 鏄 \\ue21b 渶 濂 界 殑 缁 撳 眬 銆 傛 儫 鏈 夊 \\ue6e7 姝 わ 紝 灏 辫 \\ue180 閭 ｅ 墠 涓 栦 粖 鐢 熺 殑 绾 犵 粨 灏 卞 仠 鐣 欏 湪 姝 ゅ 埢 銆 傚 啀 鐩 搁 ㈡ 椂 锛 屾 効 浠 栫 殑 浜 虹 敓 涓 嶅 啀 璁 ╀ 汉 鍞 忓 槝 锛 屼 粬 浠 \\ue102 殑 韬 \\ue0a2 績 涔 熶 細 鍙 \\ue044 眳 涓 澶 勩 傚 彲 鏄 \\ue21d 繕 鏄 \\ue21c 棝 蹇 冧 负 杩 欐 牱 鐨 勪 汉 锛 岃 繖 鏍 风 殑 鐖 扁 ︹']\n"
     ]
    }
   ],
   "source": [
    "print (cutted_train_cleaned[:2])\n",
    "print (cutted_test_cleaned[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8065\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "print (len(cutted_train_cleaned))\n",
    "print (len(cutted_test_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(x):\n",
    "    comments=[]\n",
    "    for i in range(len(x)):\n",
    "        #print (range(len(x)))\n",
    "        #print (i)\n",
    "        word_list=x[i].split(' ')\n",
    "        #print ('word list', word_list)\n",
    "        kept_word=[word for word in word_list if word not in stop_words]\n",
    "        #print ('kept word', kept_word)\n",
    "        comments.append(kept_word)\n",
    "    return comments\n",
    "train_comments_cleaned=remove_stopwords(cutted_train_cleaned)\n",
    "#print (train_comments_cleaned[-1])\n",
    "test_comments_cleaned=remove_stopwords(cutted_test_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['请问', '这机', '不是', '有个', '遥控器', '疑问'] ['请问', '这机', '不是', '有个', '遥控器', '疑问']\n",
      "['发短信', '特别', '不', '方便', '惊叹', '背后', '屏幕', '很大', '起来', '不', '舒服', '手触', '屏', '惊叹', '切换', '屏幕', '很', '麻烦', '惊叹'] ['发短信', '特别', '不', '方便', '惊叹', '背后', '屏幕', '很大', '起来', '不', '舒服', '手触', '屏', '惊叹', '切换', '屏幕', '很', '麻烦', '惊叹']\n"
     ]
    }
   ],
   "source": [
    "# 打印一下看看\n",
    "print (train_comments_cleaned[0], test_comments_cleaned[0])\n",
    "print (train_comments_cleaned[1], test_comments_cleaned[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 现在来做第二题 画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8065\n",
      "8065\n"
     ]
    }
   ],
   "source": [
    "comments_word_cnt=[]\n",
    "for i in range(len(train_comments_cleaned)):\n",
    "    comments_word_cnt.append(len(train_comments_cleaned[i]))\n",
    "print (len(comments_word_cnt))\n",
    "print (len(train_labels))\n",
    "#print (y_train[4999],y_train[5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "for my X_train dataset, the first 5000 rows are positive reviews and the rest of 2065 rows are negative review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEWCAYAAACdaNcBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xm8VXW9//HXm0lEEBWxKyBDqSWiAqKJCnLLAc00yxTDqTS8ZZfKIbW8it7yWmoaZU455FBAmF5+hkIOKCgqoOIAGqQkiF0QcMCBRD+/P9b3HBabM+wDB8/Zi/fz8diPs8bv+n7X2ue9115r7+9WRGBmZsXSoqkrYGZmjc/hbmZWQA53M7MCcribmRWQw93MrIAc7mZmBeRwb2KSRkm6vQm2K0k3S1oh6clPevvlkDRc0uQ65g+S9NInWadKJ+kFSUOauh7lqu850MjbOkLSmE9iW58Eh3uOpPMkTSyZNq+WacM+2dqVT1JPSSGpVR2L7Q8cBHSLiL0bcZsr02OBpHM3pMyIuCMiDs5tIyTtmJs/NSI+uyHbKCXpuFR3lUxvJWmJpMMbWF5Iek5Si9y0n0q6pZGqXNe2b5H00/y0iNg1IqY08nYa/dhXKX0ObEwRMQHoI2n3T2J7G5vDfW2PAPtJagkg6d+A1kD/kmk7pmXLls6Um9P+7gEsiIh3G7piPS8aW0VEe+A44AJJQ9e3gk3kLmAr4ICS6UOBAO5bjzK7AM32ZKARVR37o4H/knRQU1doPfwRGNHUlWgMzSlsmoMZZGHeN40PBh4CXiqZ9veIWAwgaV9JMyS9lf7uW1WYpCmSfibpUeA94NOSekl6WNI7kv4KbFtXhSQdKekZSW9L+ntVWKay/1vSo6msyZKqyqp64XkznUkNLCnzFOB3wMA0/6I0/duS5ktaLmmCpC65dULS6ZLmAfPq25ERMR14AehTxn46WdLLqR2vSBqemz4tDVe1aXaq87GShkhalOafK2l8STt/JWl0Gu4o6UZJr0t6LZ09t6yh3h8A44ATS2adCNwREaslbSvpHklvpn01tZ4X7l8AF9X2oihpH0mPpfJmK3fZJD1fHkn75n5JVyt3GU/SnyT9M+3XRyTtmqaPAIYDP0r76/+l6QskHSipi6T3JW2TK6ufpDcktU7j35I0V9mlu0mSetTRxvw+nEl27Kv+Z0jbu1PS0nSMR+am11qP/HMgzfucpL+m/f6SpGNy++nNquMg6XeSluTWu13SD9Jwjc+3ZArwpXLa2exFhB+5B1mY/zAN/wb4FvCzkmk3peFtgBXACUArsrPVFUCnNH8K8Cqwa5rfGpgO/BLYjOyF4h3g9lrqsjfwFtnlkxZAV+BzubL/DuwMbJ7GL03zepKdZbaqo50nA9Ny418A3gD6p7r9GngkNz+Av6Y2b15DedXbBATsR/aC9sW69hOwBfA28NlUzvbArrXUMYAdc+NDgEVpuEfa3pZpvCXwOrBPGr8buC5tbzvgSeC0WvbNfqlOm6fxjsD7QN80/j/Atel4tgYGAaqlrAB2AmYBp6ZpPwVuScNdgWXAYekYH5TGO6f504HLgTZkl9Lezj9fyJ6fHdIxuwp4JjfvFuCnJfVZAByYhh8Evp2bdxlwbRr+CjAf2CUds/OBx2ppY/WxT+P7pGNxVBpvkdp/QWrHp4GXgUPKqEf1cyAdu4XAN1Od+pM9Z6ueL68Ce6bhl9I2dsnN60cdz7fc/3SQnkeV/GjyCjS3BzAKuCsNz07/mENLpp2Uhk8AnixZfzpwchqeAlycm9cdWA1skZv2B2oP9+uAK2uZNwU4Pzf+XeC+NLzWP1st61f/06TxG4Ff5MbbAx8CPdN4AF+oo7yqbb5JFtxzgZH17af0z/Ym8DVKXjRqqGOt4Z7GpwEnpuGDyN5hAXwKWJUvn+wF5qE62jMP+EYa/jYwOzfvYuB/83Wpo5wgu4x3WAqYzVg73M8BbitZZxJwUu750i437/Y6ni9bpe11TOO3UHe4nwo8mIZFFpyD0/i9wCm59VqQBXaPeo79+2n4ctILHvB54NWSdc4Dbi6jHtXPAeBYYGoN/yMXpuHbgDOAfyML918A/wH0SnVrUdfzLZXROtW/e33Htrk/fFlmXY8A+0vamuzsaR7wGLBvmtaHNZc9ugD/KFn/H2RnY1UW5oa7ACti7evcpevn7UB2dl6bf+aG3yML5PW1VlsiYiXZGWRtbanNthGxdUTsEhGjayo7+QfQNe2LY8n+CV+X9BdJn1vPNvyBLLQBvpHGITurb53Kf1PSm2ShsF0dZd3KmkszJwC/z827jOysdnJ6e1/vzcOImEgW7qXXc3sAX6+qV6rb/mRnlF2A5RHxXm756mMgqaWkS5VdrnubLLihnkt9OePJLs11IXsXGcDUXL1+lavTcrLg7VpjSWu22x44i+yFt3WurC4lbfwx2YtuffXI6wF8vqSc4WRhDvBw2u5gsv/RKWT3Tg4ge1H4uIznW4f098062lkRHO7rmk72NnwE8ChARLwNLE7TFkfEK2nZxWRPuLzuwGu58cgNvw5sLWmLkuVrsxD4TEMbULLNcq3VllTHTtTelvUuO6neTxExKSIOIgu0F4Eb1nM7fwKGSOoGHMWacF9Idua+bURslR5bRsSudZR1K/BFZfcr9smVRUS8ExFnRsSngS8DZ0j6Yhn1Ox/4CdAuN20h2Zn7VrnHFhFxKdnzZRtJ+eV3yA1/AzgSOJDsOdszTa/6pE+dxysi3gQmA8eksv4Y6fQ11eu0knptHhGP1VPmRxFxBfAB2bvJqrJeKSmrQ0QcVkY98hYCD5eU0z4ivpPmP0x2iWxIGp5GdontgDReVce6nm+7kH3Q4O262lkJHO4lIuJ9YCbZ27v82cO0NC3/KZmJwM6SvqHso3LHAr2Be2op+x+p7IsktZG0P1k41OZG4JuSviiphaSuZZ7VLgU+Jru2Wa4/pG31lbQZcAnwREQsaEAZtal1P0n6lLLPF29BFsArgY9qKef/qKNNEbGU7GztZrIwmZumv04WHldI2jLty89IOqCOsv5Bdsz/CPw1IqrfJUk6XNKOkkR2/fajOuqcL3MK8BzZJZcqtwNflnRIOhNvq+xGcbfc82VUer4MZO3nSweyfbaM7AXjkpJN1rm/kj+QvUP5GrkXMLJ7CuflbtB2lPT1+tqYcynZzdy2ZPc33pZ0jqTNUzv7SNqrjHrk3UP2PDoh3WxtLWkvSbsApHfZ7wPHk90veptsH3yNFO5lPN8OILskVfEc7jV7mOwt+7TctKlpWnW4R8Qy4HDgTLJ/sB8Bh0fEG3WU/Q2ya5DLgQvJzhBrFBFPkt08upLsxurDrHsGXNN675HdBH40vX3dp4x1HgD+C7iT7IzxMzTSx/fq2U8t0vTFZPvkANac8ZUaBfw+temYWpb5A9mZbGlAnEh2M28O2T2B8WRnbnX5Pdn+Lj1GOwH3kwXDdOC3Uf5nx88nu2kHQEQsJDv7/jHZi/JC4GzW/G8OBwaS7befAmPJQolUr3+QvQOaAzxesq0bgd5pf91dS30mpPb8X0TMztXrLuDnwJh0yed54NAy2wjwF7L9/O2I+IjsRakv8ArZTdDfkb3bqLMeeRHxDnAw2fNyMdllyZ+T3ceo8jCwLCJezY0LeDqN1/d8O47skl3Fq7rhYWYVQNJY4MWIuLCp61I0kr4MnBARtZ04VBSHu1kzli5dLCc74z2Y7COdAyPi6TpXtE1eXd80NLOm92/An8lubi8CvuNgt3L4zN3MrIB8Q9XMrICa7LLMtttuGz179myqzZuZVaRZs2a9ERGd61uuycK9Z8+ezJw5s6k2b2ZWkSTV9a32ar4sY2ZWQA53M7MCcribmRWQP+duViE+/PBDFi1axAcffNDUVbFPQNu2benWrRutW7euf+EaONzNKsSiRYvo0KEDPXv2RGv/xKsVTESwbNkyFi1aRK9evdarDF+WMasQH3zwAZ06dXKwbwIk0alTpw16l+ZwN6sgDvZNx4Yea4e7mVkB+Zq7WYUaNeqTL69ly5bsttturF69ml122YXf//73tGvXrv4Vc0499VTOOOMMevfuzSWXXMKPf/zj6nn77rsvjz1W5489lSVfz169enHbbbex1VZbNbicxYsXM3LkSMaPH7/BdfqkNVnHYQMGDIgN+YZqYz+xm/t2zebOncsuu+xSPd4U4d6+fXtWrlwJwPDhw9lzzz0544wz1nub+fIaU77ck046iZ133pmf/OQnjb6dja30mANImhURA+pb15dlzGy9DBo0iPnz5wPwy1/+kj59+tCnTx+uuuoqAN59912+9KUvsccee9CnTx/Gjh0LwJAhQ5g5cybnnnsu77//Pn379mX48OFAFsoAxx57LBMnTqze1sknn8ydd97JRx99xNlnn81ee+3F7rvvznXX1f+jSQMHDuS119b8FPBll11Wvf6FF2a/eXLOOefw29/+tnqZUaNGccUVV7BgwQL69OkDUOu2v/vd7zJhwgQAjjrqKL71rW8BcOONN3L++efXuh82Noe7mTXY6tWruffee9ltt92YNWsWN998M0888QSPP/44N9xwA08//TT33XcfXbp0Yfbs2Tz//PMMHTp0rTIuvfRSNt98c5555hnuuOOOteYNGzasOgT/9a9/8cADD3DYYYdx44030rFjR2bMmMGMGTO44YYbeOWVV6jNRx99xAMPPMARRxwBwOTJk5k3bx5PPvkkzzzzDLNmzeKRRx5Za3sA48aN4+tfX/snY2vb9uDBg5k6Nfu55ddee405c+YAMG3aNAYNGlTvfthYHO5mVraqM+0BAwbQvXt3TjnlFKZNm8ZRRx3FFltsQfv27fnqV7/K1KlT2W233bj//vs555xzmDp1Kh07dqx/A8mhhx7Kgw8+yKpVq7j33nsZPHgwm2++OZMnT+bWW2+lb9++fP7zn2fZsmXMmzev1np26tSJ5cuXc9BBBwFZuE+ePJl+/frRv39/XnzxRebNm0e/fv1YsmQJixcvZvbs2Wy99dZ07959rTJr2/agQYOYOnUqc+bMoXfv3nzqU5/i9ddfZ/r06ey7774btB82RGXeUB01iiFTmmzjTbVhsyZXdaadV9t9u5133plZs2YxceJEzjvvPA4++GAuuOCCsrbTtm1bhgwZwqRJkxg7dizHHXdc9bZ+/etfc8ghh5RVz7feeovDDz+cq6++mpEjRxIRnHfeeZx22mnrrHP00Uczfvx4/vnPfzJs2Lq/DV/XtlesWMF9993H4MGDWb58OePGjaN9+/Z06NCBDh06rPd+2BA+czezDTJ48GDuvvtu3nvvPd59913uuusuBg0axOLFi2nXrh3HH388Z511Fk899dQ667Zu3ZoPP/ywxnKHDRvGzTffzNSpU6sD9ZBDDuGaa66pXudvf/sb7777bq1169ixI6NHj+byyy/nww8/5JBDDuGmm26qvtn62muvsWTJkurtjRkzhvHjx3P00UevU1Zd2x44cCBXXXUVgwcPZtCgQVx++eUMGjQIoKz9sDFU5pm7mTWbT27179+fk08+mb333hvIPurYr18/Jk2axNlnn02LFi1o3bo111xzzTrrjhgxgt13353+/fuvc9394IMP5sQTT+SII46gTZs21WUvWLCA/v37ExF07tyZu+++u8769evXjz322IMxY8ZwwgknMHfuXAYOHAhkN3Bvv/12tttuO3bddVfeeecdunbtyvbbb79OOXVte9CgQUyePJkdd9yRHj16sHz58upwf+655+rdDxtDZX4UctQopkxp1OqUbciUUU2zYdvk1fSxOCs2fxTSzMzW4nA3Mysgh7uZWQE53M3MCsjhbmZWQA53M7MC8ufczSpVE3QLKYkzzjiDK664AoDLL7+clStXMqqR6+KugDecz9zNrGybbbYZf/7zn3njjTc26nYuueSStcYbI9hhTbcEzz//PNtssw1XX331epXTpUuXZh3s4HA3swZo1aoVI0aM4Morr1xn3tKlS/na177GXnvtxV577cWjjz5aPf2ggw6if//+nHbaafTo0aP6xeErX/kKe+65J7vuuivXX389gLsCbiRlhbukoZJekjRf0rk1zO8u6SFJT0t6VtJhjVpLM2s2Tj/9dO644w7eeuuttaZ///vf54c//CEzZszgzjvv5NRTTwXgoosu4gtf+AJPPfUURx11FK+++mr1OjfddBOzZs1i5syZjB49mmXLlrkr4EZS7zV3SS2Bq4GDgEXADEkTImJObrHzgXERcY2k3sBEoGej1tTMmoUtt9ySE088kdGjR7P55ptXT7///vurAwzg7bff5p133mHatGncddddAAwdOpStt966epnRo0dXz1u4cCHz5s2jU6dOtW770EMPZeTIkaxataq6F8aqroCfffbZ6kslb731FvPmzaNXr15rrV/1jmDBggXsueeeNXYFDLBy5UrmzZvHKaecUt0V8NKlS6u7Al6wYEF1mbVte9CgQVx11VXVXQGvWLGiuivg0aNH8/rrr3PWWWdxzjnncPjhh1f3RdNYyrmhujcwPyJeBpA0BjgSyId7AFum4Y7A4saspJk1Lz/4wQ/o378/3/zmN6unffzxx0yfPn2twIfauwSeMmUK999/P9OnT6ddu3YMGTKEDz74oM7tuivg8pVzWaYrsDA3vihNyxsFHC9pEdlZ+3/WVJCkEZJmSpq5dOnS9aiumTUH22yzDccccww33nhj9bSDDz6Y3/zmN9XjVf2+77///owbNw7IznJXrFgBZGe4W2+9Ne3atePFF1/k8ccfr17XXQFvuHLO3FXDtNKX4uOAWyLiCkkDgdsk9YmIj9daKeJ64HrIeoVcnwqbWdLEff6eeeaZa4X56NGjOf3009l9991ZvXo1gwcP5tprr+XCCy/kuOOOY+zYsRxwwAFsv/32dOjQgaFDh3Lttdey++6789nPfpZ99tmnuix3Bbzh6u3yN4X1qIg4JI2fBxAR/5Nb5gVgaEQsTOMvA/tExJLaynWXv2YNU6ld/q5atYqWLVvSqlUrpk+fzne+8511fs3JarYhXf6Wc+Y+A9hJUi/gNWAY8I2SZV4FvgjcImkXoC3g6y5mxquvvsoxxxzDxx9/TJs2bbjhhhuaukqbhHrDPSJWS/oeMAloCdwUES9IuhiYGRETgDOBGyT9kOySzcnRVL8CYmbNyk477cTTTz/d1NXY5JTV/UBETCS7UZqfdkFueA6wX+NWzcxKRQRSTbfBrGg29PzY31A1qxBt27Zl2bJlG/xPb81fRLBs2TLatm273mW44zCzCtGtWzcWLVqEP0a8aWjbti3dunVb7/Ud7mYVonXr1ut849KsNr4sY2ZWQA53M7MCcribmRWQw93MrIAc7mZmBeRwNzMrIIe7mVkBOdzNzArI4W5mVkAOdzOzAnK4m5kVkMPdzKyAHO5mZgXkcDczKyCHu5lZATnczcwKyOFuZlZADnczswJyuJuZFZDD3cysgBzuZmYF5HA3Mysgh7uZWQE53M3MCsjhbmZWQA53M7MCcribmRWQw93MrIAc7mZmBeRwNzMrIIe7mVkBOdzNzArI4W5mVkAOdzOzAior3CUNlfSSpPmSzq1lmWMkzZH0gqQ/NG41zcysIVrVt4CklsDVwEHAImCGpAkRMSe3zE7AecB+EbFC0nYbq8JmZla/cs7c9wbmR8TLEfEvYAxwZMky3waujogVABGxpHGraWZmDVFOuHcFFubGF6VpeTsDO0t6VNLjkoY2VgXNzKzh6r0sA6iGaVFDOTsBQ4BuwFRJfSLizbUKkkYAIwC6d+/e4MqamVl5yjlzXwTskBvvBiyuYZn/jYgPI+IV4CWysF9LRFwfEQMiYkDnzp3Xt85mZlaPcsJ9BrCTpF6S2gDDgAkly9wN/DuApG3JLtO83JgVNTOz8tUb7hGxGvgeMAmYC4yLiBckXSzpiLTYJGCZpDnAQ8DZEbFsY1XazMzqVs41dyJiIjCxZNoFueEAzkgPMzNrYv6GqplZATnczcwKyOFuZlZADnczswJyuJuZFZDD3cysgBzuZmYF5HA3Mysgh7uZWQE53M3MCsjhbmZWQA53M7MCcribmRWQw93MrIAc7mZmBeRwNzMrIIe7mVkBOdzNzArI4W5mVkAOdzOzAnK4m5kVkMPdzKyAHO5mZgXkcDczKyCHu5lZATnczcwKyOFuZlZADnczswJyuJuZFZDD3cysgBzuZmYF5HA3Mysgh7uZWQE53M3MCsjhbmZWQA53M7MCcribmRWQw93MrIDKCndJQyW9JGm+pHPrWO5oSSFpQONV0czMGqrecJfUErgaOBToDRwnqXcNy3UARgJPNHYlzcysYco5c98bmB8RL0fEv4AxwJE1LPffwC+ADxqxfmZmth7KCfeuwMLc+KI0rZqkfsAOEXFPXQVJGiFppqSZS5cubXBlzcysPOWEu2qYFtUzpRbAlcCZ9RUUEddHxICIGNC5c+fya2lmZg1STrgvAnbIjXcDFufGOwB9gCmSFgD7ABN8U9XMrOmUE+4zgJ0k9ZLUBhgGTKiaGRFvRcS2EdEzInoCjwNHRMTMjVJjMzOrV73hHhGrge8Bk4C5wLiIeEHSxZKO2NgVNDOzhmtVzkIRMRGYWDLtglqWHbLh1TIzsw3hb6iamRWQw93MrIAc7mZmBeRwNzMrIIe7mVkBOdzNzArI4W5mVkAOdzOzAnK4m5kVkMPdzKyAHO5mZgXkcDczKyCHu5lZATnczcwKyOFuZlZADnczswJyuJuZFZDD3cysgBzuZmYF5HA3Mysgh7uZWQE53M3MCsjhbmZWQA53M7MCcribmRWQw93MrIAc7mZmBeRwNzMrIIe7mVkBOdzNzArI4W5mVkAOdzOzAnK4m5kVkMPdzKyAHO5mZgXkcDczKyCHu5lZATnczcwKqKxwlzRU0kuS5ks6t4b5Z0iaI+lZSQ9I6tH4VTUzs3LVG+6SWgJXA4cCvYHjJPUuWexpYEBE7A6MB37R2BU1M7PylXPmvjcwPyJejoh/AWOAI/MLRMRDEfFeGn0c6Na41TQzs4YoJ9y7Agtz44vStNqcAtxb0wxJIyTNlDRz6dKl5dfSzMwapJxwVw3TosYFpeOBAcBlNc2PiOsjYkBEDOjcuXP5tTQzswZpVcYyi4AdcuPdgMWlC0k6EPgJcEBErGqc6pmZ2foo58x9BrCTpF6S2gDDgAn5BST1A64DjoiIJY1fTTMza4h6wz0iVgPfAyYBc4FxEfGCpIslHZEWuwxoD/xJ0jOSJtRSnJmZfQLKuSxDREwEJpZMuyA3fGAj18vMzDaAv6FqZlZADnczswJyuJuZFZDD3cysgBzuZmYF5HA3Mysgh7uZWQE53M3MCsjhbmZWQA53M7MCcribmRWQw93MrIAc7mZmBeRwNzMrIIe7mVkBOdzNzArI4W5mVkAOdzOzAnK4m5kVUFm/oWo5o0ZtWts1s4rkM3czswJyuJuZFZDD3cysgBzuZmYF5BuqDTRlStNsd0jTbNbMKpTP3M3MCsjhbmZWQA53M7MCcribmRWQw93MrIAc7mZmBeRwNzMrIIe7mVkBOdzNzArI4W5mVkAOdzOzAnLfMpWiKX+swz8UYlZxHO4Vosk6LBvSNNs1sw1T1mUZSUMlvSRpvqRza5i/maSxaf4Tkno2dkXNzKx89Z65S2oJXA0cBCwCZkiaEBFzcoudAqyIiB0lDQN+Dhy7MSpsn6wpU4Aho5pk20OmNM12zYqgnMsyewPzI+JlAEljgCOBfLgfCYxKw+OB30hSREQj1tU2MVOa6EVlU+R9/cn5pG5hlRPuXYGFufFFwOdrWyYiVkt6C+gEvJFfSNIIYEQaXSnppfWpNLBtadkFU/T2QfHbWFnte/iihq5RWe1ruI3WvosavKvX0aOchcoJd9UwrfSMvJxliIjrgevL2GbdFZJmRsSADS2nuSp6+6D4bXT7KlsR2lfODdVFwA658W7A4tqWkdQK6Agsb4wKmplZw5UT7jOAnST1ktQGGAZMKFlmAnBSGj4aeNDX283Mmk69l2XSNfTvAZOAlsBNEfGCpIuBmRExAbgRuE3SfLIz9mEbs9I0wqWdZq7o7YPit9Htq2wV3z75BNvMrHjct4yZWQE53M3MCqjiwr2+rhAqgaQdJD0kaa6kFyR9P03fRtJfJc1Lf7dO0yVpdGrzs5L6N20LyiOppaSnJd2Txnul7inmpe4q2qTpFdd9haStJI2X9GI6jgOLdPwk/TA9N5+X9EdJbSv5+Em6SdISSc/npjX4eEk6KS0/T9JJNW2ruaiocM91hXAo0Bs4TlLvpq3VelkNnBkRuwD7AKendpwLPBAROwEPpHHI2rtTeowArvnkq7xevg/MzY3/HLgytW8FWbcVkOu+ArgyLdfc/Qq4LyI+B+xB1s5CHD9JXYGRwICI6EP2QYqqbkUq9fjdAgwtmdag4yVpG+BCsi9x7g1cWPWC0CxFRMU8gIHApNz4ecB5TV2vRmjX/5L13fMSsH2atj3wUhq+Djgut3z1cs31QfZ9iAeALwD3kH3R7Q2gVemxJPsk1sA03Cotp6ZuQx1t2xJ4pbSORTl+rPnG+TbpeNwDHFLpxw/oCTy/vscLOA64Ljd9reWa26OiztypuSuErk1Ul0aR3sL2A54APhURrwOkv9ulxSqx3VcBPwI+TuOdgDcjYnUaz7dhre4rgKruK5qrTwNLgZvTZaffSdqCghy/iHgNuBx4FXid7HjMojjHr0pDj1dFHcdKC/eyujmoFJLaA3cCP4iIt+tatIZpzbbdkg4HlkTErPzkGhaNMuY1R62A/sA1EdEPeJc1b+lrUlHtS5cajgR6AV2ALcguVZSq1ONXn9raU1HtrLRwL6crhIogqTVZsN8REX9Ok/9P0vZp/vbAkjS90tq9H3CEpAXAGLJLM1cBW6XuKWDtNlRa9xWLgEUR8UQaH08W9kU5fgcCr0TE0oj4EPgzsC/FOX5VGnq8Kuo4Vlq4l9MVQrMnSWTf6p0bEb/Mzcp343AS2bX4quknprv4+wBvVb2dbI4i4ryI6BYRPcmO0YMRMRx4iKx7Cli3fRXTfUVE/BNYKOlxZaVtAAAGY0lEQVSzadIXybrALsTxI7scs4+kdum5WtW+Qhy/nIYer0nAwZK2Tu9uDk7Tmqemvui/HjdFDgP+Bvwd+ElT12c927A/2du5Z4Fn0uMwsuuUDwDz0t9t0vIi+5TQ34HnyD7F0OTtKLOtQ4B70vCngSeB+cCfgM3S9LZpfH6a/+mmrncZ7eoLzEzH8G5g6yIdP+Ai4EXgeeA2YLNKPn7AH8nuH3xIdgZ+yvocL+BbqZ3zgW82dbvqerj7ATOzAqq0yzJmZlYGh7uZWQE53M3MCsjhbmZWQA53M7MCcrhvYiSFpCty42dJGtWEVaqVpMtSz4SX5aZJ0hu5Hvy2T23aP7fMUknr/fV3SQskbVvmshdLOrABZfeVdFhufJSks9anno2hofW3yuFw3/SsAr5abng1ttw3HMtxGtA/Is6umhDZZ3efIOu4CrJvTj6d/pK+WPRGRCzbCPVZR0RcEBH3N2CVvmTfaWh0qdfUBlmP+luFcLhvelaT/T7kD0tnSLpF0tG58ZXp7xBJD0saJ+lvki6VNFzSk5Kek/SZtFxnSXdKmpEe+6XpoyRdL2kycGvJNpXO0J9PZR2bpk8g69PkiappOY+Swjz9/SVrh/1jqYwekh5IfXI/IKl7rp2/lPQQ8HNJnSRNTp2AXUfqQ0TSFpL+Iml2ql9pPdbaZ+mM/yJJT6W2fK5k2TbAxcCxkp7Jlddb0hRJL0samVv++LSPn5F0XU3hnbZ5gaRpwNclfUbSfZJmSZoq6XOSOqblWqR12klaKKl1Sf33TMd5lqRJ6V3RdpJmpfl7pHdJVfvx75LaldbJmgeH+6bpamC4pI4NWGcPsv7ZdwNOAHaOiL2B3wH/mZb5FVl/33sBX0vzquwJHBkR3ygp96tkZ7N7kPVpcpmk7SPiCOD9iOgbEWNL1nmMNeG+N9k3RKv6/NiXLPwBfgPcGhG7A3cAo3Nl7AwcGBFnkvXRPS2yTsAmAN3TMkOBxRGxR2T9mt9X304ie9fQn6wP8LUut0TEv4ALgLEl7focWZe6VX2Et5a0C3AssF9E9AU+AobXss0PImL/iBhD9sL9nxGxZ9r+byPiLWA2cEBa/stk3fV+WFWAsr6Ofg0cnda9CfhZRCwB2kraEhhE9q3cQZJ6kHUO914Z+8SawAa9JbXKFBFvS7qV7AcZ3i9ztRmR+kOR9Hdgcpr+HPDvafhAsrPQqnW2lNQhDU+IiJq2tT/wx4j4iKwjp4eBvai7z6AngX7KutltHREr01nvjmThXnVPYSDZiwdkX6H/Ra6MP6VtAgyuWi4i/iJpRa5tl0v6OVkXClPrqFOVqk7gZuW2XZ+/RMQqYJWkJcCnyPpz2ROYkfbn5qzp2KrUWKjuZXRf4E+5Y7BZbpljyfqHGQb8tqSMzwJ9gL+mdVuSfV0fshfT/cj20yVkL3oCytkf1kQc7puuq4CngJtz01aT3s0p+w9vk5u3Kjf8cW78Y9Y8j1qQ/WjDWiGewuLdWupRUzeqdYqI9yTNJ+vn46k0+XGya9nbkf24Qo2r5oZL67NOPxwR8TdJe6Zy/0fS5Ii4uJ7qVe2Xjyj//yu/b6vWE/D7iDivjPWr2tKCrM/1vjUsM4GsDduQvWg8WDJfwAsRMXCdNbMQHwT0IOtc6xyy/XVPGXWzJuLLMpuoiFgOjGPNT6UBLCD7x4esP+/WDSx2MvC9qhFJNYVMqUfIrkG3lNSZ7OzwyTLWexT4ATA9jU8nu2z0eKzpMOkxsrNUyC5pTKujDsNTnQ8l6wQMSV2A9yLidrIfr2iM3z59B+hQ71JZR1ZHS9ou1WWbdCmkVpH9JsArkr6e1pGkPdK8lWT79Vdk70I+Kln9JaCzpIFp3daSdk3zHgGOB+ZFxMdk3fkexprLX9YMOdw3bVcA+U/N3AAcIOlJst+JrO1suzYjgQHpBuYc4D/KWOcusp4VZ5OdTf4osi516/MoWS+FVeH+FFn/2o+V1Oebkp4lu0/w/VrKuggYLOkpsm5cX03TdwOelPQM8BPgp2XUqz4PkV26yt9QXUdEzAHOByan+v+V7Kfe6jMcOEXSbOAFshfpKmPJQrr0HkbV/YCjyW4wzybrqXTfNG9BWuyR9Hca2TuEFaXlWPPhXiHNzArIZ+5mZgXkcDczKyCHu5lZATnczcwKyOFuZlZADnczswJyuJuZFdD/ByYXCYoFk5UTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "num_bins = 80\n",
    "#n, bins, patches = plt.hist(comments_word_cnt[:5000], num_bins, facecolor='blue', alpha=0.5, label='Positive Reviews')\n",
    "#n, bins, patches = plt.hist(comments_word_cnt[5000:], num_bins, facecolor='red', alpha=0.5,label='Negative Reviews')\n",
    "x=comments_word_cnt[:5000]\n",
    "y=comments_word_cnt[5000:]\n",
    "np.ones(len(x)) / len(x)\n",
    "plt.hist(x, weights=np.ones(len(x)) / len(x),  facecolor='blue', alpha=0.5, label='Positive Reviews')\n",
    "plt.hist(y, weights=np.ones(len(y)) / len(y),  facecolor='red', alpha=0.5,label='Negative Reviews')\n",
    "plt.title('Word cnt for Positive Vs Negative Reviews)')\n",
    "plt.xlabel('Numer of Words in the review')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=green>画了一个正负 review 的 histogram，通过这个图可以看出，正的review 在字数少的bin里面百分比稍微多了一点点\n",
    "所以正面的review 相对来说比较短，负面的稍微长一些,但是差别没有那么明显。\n",
    "<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4. 把文本转换成向量\n",
    "> 预处理好文本之后，我们就需要把它转换成向量形式，这里我们使用tf-idf的方法。 sklearn自带此功能，直接调用即可。输入就是若干个文本，输出就是每个文本的tf-idf向量。详细的使用说明可以在这里找到： 参考：https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html  这里需要特别注意的一点是：对于训练数据调用fit_transform, 也就是训练的过程。 但对于测试数据，不能再做训练，而是直接使用已经训练好的object做transform操作。思考一下为什么要这么做？\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8065 2500\n",
      "(8065, 2851) (2500, 2851) (8065,) (2500,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: 利用tf-idf从文本中提取特征,写到数组里面. \n",
    "#       参考：https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#prepare the input as cropus format\n",
    "train_corpus=[]\n",
    "for i in range(len(train_comments_cleaned)):\n",
    "    train_corpus.append(' '.join(train_comments_cleaned[i]) )\n",
    "\n",
    "test_corpus=[]\n",
    "for i in range(len(test_comments_cleaned)):\n",
    "    test_corpus.append(' '.join(test_comments_cleaned[i]) )    \n",
    "print (len(train_corpus), len(test_corpus))\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_corpus)\n",
    "y_train =train_labels \n",
    "X_test = vectorizer.transform(test_corpus)\n",
    "y_test =test_labels \n",
    "\n",
    "print (np.shape(X_train), np.shape(X_test), np.shape(y_train), np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5. 通过交叉验证来训练模型\n",
    "> 接下来需要建模了！ 这里我们分别使用逻辑回归，朴素贝叶斯和SVM来训练。针对于每一个方法我们使用交叉验证（gridsearchCV)， 并选出最好的参数组合，然后最后在测试数据上做验证。 这部分已经在第二次作业中讲过。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "params_c = np.logspace(-4, 1, 11)\n",
    "parameter_grid = {'C': params_c}\n",
    "model= LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "Best parameters: {'C': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO： 利用逻辑回归来训练模型\n",
    "#       1. 评估方式： F1-score\n",
    "#       2. 超参数（hyperparater）的选择利用grid search https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "#       3. 打印出在测试数据中的最好的结果（precision, recall, f1-score, 需要分别打印出正负样本，以及综合的）\n",
    "#       请注意：做交叉验证时绝对不能用测试数据。 测试数据只能用来最后的”一次性“检验。\n",
    "#       逻辑回归的使用方法请参考：http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "#       对于逻辑回归，经常调整的超参数为： C\n",
    "%time\n",
    "grid_search = GridSearchCV(model,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=4, n_jobs=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "\n",
      "0.7654037421397959\n",
      "\n",
      "[mean: 0.76540, std: 0.00010, params: {'C': 0.0001}, mean: 0.76540, std: 0.00010, params: {'C': 0.00031622776601683794}, mean: 0.76540, std: 0.00010, params: {'C': 0.001}, mean: 0.76540, std: 0.00010, params: {'C': 0.0031622776601683794}, mean: 0.76540, std: 0.00010, params: {'C': 0.01}, mean: 0.76527, std: 0.00021, params: {'C': 0.03162277660168379}, mean: 0.76481, std: 0.00138, params: {'C': 0.1}, mean: 0.76273, std: 0.00335, params: {'C': 0.31622776601683794}, mean: 0.75801, std: 0.00148, params: {'C': 1.0}, mean: 0.74787, std: 0.00250, params: {'C': 3.1622776601683795}, mean: 0.74351, std: 0.00384, params: {'C': 10.0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "print (grid_search.best_estimator_)\n",
    "print()\n",
    "print (grid_search.best_score_)\n",
    "print()\n",
    "print (grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9030    0.7044    0.7914      3065\n",
      "          1     0.8403    0.9536    0.8934      5000\n",
      "\n",
      "avg / total     0.8641    0.8589    0.8546      8065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_train_predict = grid_search.predict(X_train)\n",
    "print (classification_report(y_train, y_train_predict,digits=4 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. 打印出在测试数据中的最好的结果（precision, recall, f1-score, 需要分别打印出正负样本，以及综合的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8513    0.5360    0.6578      1250\n",
      "          1     0.6614    0.9064    0.7648      1250\n",
      "\n",
      "avg / total     0.7564    0.7212    0.7113      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_test_predict = grid_search.predict(X_test)\n",
    "print (classification_report(y_test, y_test_predict,digits=4 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9085    0.3176    0.4707      1250\n",
      "          1     0.5865    0.9680    0.7305      1250\n",
      "\n",
      "avg / total     0.7475    0.6428    0.6006      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.Se import MultinomialNB\n",
    "# TODO： 利用朴素贝叶斯来训练模型\n",
    "#       1. 评估方式： F1-score\n",
    "#       2. 超参数（hyperparater）的选择利用grid search https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "#       3. 打印出在测试数据中的最好的结果（precision, recall, f1-score, 需要分别打印出正负样本，以及综合的）\n",
    "#       请注意：做交叉验证时绝对不能用测试数据。 测试数据只能用来最后的”一次性“检验。\n",
    "#       朴素贝叶斯的使用方法请参考：https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB\n",
    "#       对于朴素贝叶斯，一般不太需要超参数的调节。但如果想调参，也可以参考上面的链接，有几个参数是可以调节的。 \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_test_predict = clf.predict(X_test)\n",
    "print (classification_report(y_test, y_test_predict,digits=4 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 9.3 µs\n",
      "Best parameters: {'C': 1.0, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "# TODO： 利用SVM来训练模型\n",
    "#       1. 评估方式： F1-score\n",
    "#       2. 超参数（hyperparater）的选择利用grid search https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "#       3. 打印出在测试数据中的最好的结果（precision, recall, f1-score, 需要分别打印出正负样本，以及综合的）\n",
    "#       请注意：做交叉验证时绝对不能用测试数据。 测试数据只能用来最后的”一次性“检验。\n",
    "#       SVM的使用方法请参考：http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "#       对于SVM模型，经常调整的超参数为：C, gamma, kernel。 这里的参数C跟逻辑回归是一样的， gamma和kernel是针对于SVM的参数\n",
    "#       在这里先不要考虑他们的含义（或者通过官方文档试图理解一下）， 在课程最后的部分会讲到这些内容。 \n",
    "from sklearn.svm import SVC\n",
    "params_c = np.logspace(-4, 1, 11)\n",
    "kernel_list=['linear', 'poly', 'rbf', 'sigmoid']\n",
    "parameter_grid = {'C': params_c,\n",
    "                'kernel': kernel_list}\n",
    "model = SVC()\n",
    "%time\n",
    "grid_search = GridSearchCV(model, param_grid=parameter_grid, cv=4, n_jobs=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "0.8173056135779905\n"
     ]
    }
   ],
   "source": [
    "print (grid_search.best_estimator_)\n",
    "print (grid_search.best_score_)\n",
    "#print (grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9414    0.8232    0.8783      3065\n",
      "          1     0.8994    0.9686    0.9327      5000\n",
      "\n",
      "avg / total     0.9153    0.9133    0.9120      8065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#score from train set\n",
    "from sklearn.metrics import classification_report\n",
    "y_train_predict = grid_search.predict(X_train)\n",
    "print (classification_report(y_train, y_train_predict,digits=4 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8297    0.5728    0.6777      1250\n",
      "          1     0.6738    0.8824    0.7641      1250\n",
      "\n",
      "avg / total     0.7517    0.7276    0.7209      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#score  from test set\n",
    "from sklearn.metrics import classification_report\n",
    "y_test_predict = grid_search.predict(X_test)\n",
    "print (classification_report(y_test, y_test_predict,digits=4 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> 对于超参数的调整，我们经常使用gridsearch，这也是工业界最常用的方法，但它的缺点是需要大量的计算，所以近年来这方面的研究也成为了重点。 其中一个比较经典的成果为Bayesian Optimization（利用贝叶斯的思路去寻找最好的超参数）。Ryan P. Adams主导的Bayesian Optimization利用高斯过程作为后验概率（posteior distribution）来寻找最优解。 https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf 在下面的练习中，我们尝试使用Bayesian Optimization工具来去寻找最优的超参数。参考工具：https://github.com/fmfn/BayesianOptimization  感兴趣的朋友可以去研究一下。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 6. 思考题 \n",
    "1. 对于情感分析来说，有一个问题也很重要，比如一个句子里出现了 “我不太兴奋”， “不是很满意”。 在这种情况，因为句子中出现了一些积极的词汇很可能被算法识别成正面的，但由于前面有一个“不”这种关键词，所以否定+肯定=否定，算法中这种情况也需要考虑。另外，否定+否定=肯定， 这种情况也一样。 \n",
    "2. 另外一个问题是aspect-based sentiment analysis, 这个指的是做情感分析的时候，我们既想了解情感，也想了解特定的方面。 举个例子： “这部手机的电池性能不错，但摄像不够清晰啊!”, 分析完之后可以得到的结论是： “电池：正面， 摄像：负面”， 也就是针对于一个产品的每一个性能做判定，这种问题我们叫做aspect-based sentiment analysis，也是传统情感分析的延伸。\n",
    "\n",
    ">``Q``: 对于如上两个问题，有什么解决方案？ 大概列一下能想到的处理方案。 用简介的文字来描述即可。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "// 你的答案在这里.......\n",
    "1. 在不后面的词，word2vec转换的时候要调整里面的数字，变成（1-原来的数字）？\n",
    "\n",
    "2.找出句子中含有转折词的“但是”，“尽管”，然后划分成两个句子来处理\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 7. 其他领域（仅供参考）\n",
    "跟情感分析类似的领域有叫affective computing, 也就是用来识别情绪(emotion recognition)。但情感和情绪又不太一样，情绪指的是高兴，低落，失落，兴奋这些人的情绪。我们知道真正的人工智能是需要读懂人类的情绪的。而且情绪识别有很多场景，比如服务机器人根据不同的情绪来跟用户交流； 无人驾驶里通过识别用户的情绪（摄像头或者声音或者传感器）来保证安全驾驶； IOT领域里设备也需要读懂我们的情绪； 微博里通过文本读懂每个人发文时的情绪。 \n",
    "\n",
    "总体来讲，情绪识别跟情感识别所用到的技术是类似的，感兴趣的小伙伴，也可以关注一下这个领域。 如果想发论文，强烈建议选择情绪方面的，不建议选择情感分析，因为问题太老了。情绪分析是近几年才开始受关注的领域。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=#FF0000 size=4 face=\"黑体\"> 很棒，超出作业完成设计，期待你的坚持</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
